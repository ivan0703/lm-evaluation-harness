"""
SAMSum Corpus: A Human-annotated Dialogue Dataset for Abstractive Summarization
https://arxiv.org/abs/1911.12237v2s

"""

import numpy as np
import datasets
from rouge_score import rouge_scorer, scoring
from lm_eval.base import rf, Task
from lm_eval.metrics import mean

SUMMARIZE_PROMPT = "summarize: "

class Samsum(Task):
    VERSION = 1
    DATASET_PATH = "../samsum/samsum.py"
    DATASET_NAME = None

    def __init__(self):
        super().__init__()

    def has_training_docs(self):
        return True

    def has_validation_docs(self):
        return True

    def has_test_docs(self):
        return True

    def training_docs(self):
        return self.dataset["train"]

    def validation_docs(self):
        return self.dataset["validation"]
    
    def test_docs(self):
        return self.dataset["test"]

    def doc_to_text(self, doc):
        return SUMMARIZE_PROMPT + "\n" + doc["dialogue"]

    def doc_to_target(self, doc):
        return " "

    def fewshot_context(
        self, doc, num_fewshot, provide_description=None, rnd=None, description=None
    ):
        assert (
            num_fewshot == 0
        ), "TruthfulQA is intended only for the zero-shot setting."
        return super().fewshot_context(
            doc=doc, num_fewshot=num_fewshot, rnd=rnd, description=description
        )

    def construct_requests(self, doc, ctx):
        """Uses RequestFactory to construct Requests and returns an iterable of
        Requests which will be sent to the LM.

        :param doc:
            The document as returned from training_docs, validation_docs, or test_docs.
        :param ctx: str
            The context string, generated by fewshot_context. This includes the natural
            language description, as well as the few shot examples, and the question
            part of the document for `doc`.
        """
        # TODO: Find a way to cap the number of generated tokens to `50` as in the official implementation.
        completion = rf.greedy_until(ctx, {"until": ["."]})
        return completion

    def process_results(self, doc, results):
        """Take a single document and the LM results and evaluates, returning a
        dict where keys are the names of submetrics and values are the values of
        the metric for that one document

        :param doc:
            The document as returned from training_docs, validation_docs, or test_docs.
        :param results:
            The results of the requests created in construct_requests.
        """
        completion = results[0].strip()
        ref = doc["summary"]
        
        # ROUGE-N
        rouge_scores = self.rouge([ref], [completion])
        # ROUGE-1
        rouge1_score = rouge_scores["rouge1"]
        # ROUGE-2
        rouge2_score = rouge_scores["rouge2"]
        # ROUGE-L
        rougeL_score = rouge_scores["rougeLsum"]
       

        return {
            "rouge1": rouge1_score,            
            "rouge2": rouge1_score,
            "rougeL": rougeL_score,
        }

    def aggregation(self):
        return {
            "rouge1": mean,
            "rouge2": mean,
            "rougeL": mean,
        }

    def higher_is_better(self):
        return {
            "rouge1": True,
            "rouge2": True,
            "rougeL": True,
        }

    def rouge(self, refs, preds):
        """
        Returns `t5` style ROUGE scores. See the related implementation:
        https://github.com/google-research/text-to-text-transfer-transformer/blob/3d10afd51ba97ac29eb66ae701eca274488202f7/t5/evaluation/metrics.py#L68

        :param refs:
            A `list` of reference `strs`.
        :param preds:
            A `list` of predicted `strs`.
        """
        rouge_types = ["rouge1", "rouge2", "rougeLsum"]
        scorer = rouge_scorer.RougeScorer(rouge_types)
        # Add newlines between sentences to correctly compute `rougeLsum`.

        def _prepare_summary(summary):
            summary = summary.replace(" . ", ".\n")
            return summary

        # Accumulate confidence intervals.
        aggregator = scoring.BootstrapAggregator()
        for ref, pred in zip(refs, preds):
            ref = _prepare_summary(ref)
            pred = _prepare_summary(pred)
            aggregator.add_scores(scorer.score(ref, pred))
        result = aggregator.aggregate()
        return {type: result[type].mid.fmeasure * 100 for type in rouge_types}